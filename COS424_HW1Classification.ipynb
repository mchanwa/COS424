{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COS424-HW1Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y6GoBeftonJa"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mchanwa/COS424/blob/main/COS424_HW1Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6GoBeftonJa"
      },
      "source": [
        "# Preprocessing Code for Twitter Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SarfCQ5JR02N",
        "outputId": "76761d9f-3d17-4187-e40d-3026149f3433"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKW_ND7xwJqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee4d9b6-de2f-476f-acf7-c07ec813c0ce"
      },
      "source": [
        "!pip install emoji\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import string\n",
        "import nltk\n",
        "import emoji\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) - {'all'}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cunU8zjb5CTB"
      },
      "source": [
        "# Gets the part of speech tag of word for lemmatization\n",
        "# This function is based on code from:\n",
        "#   https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Preprocesses the tweets text\n",
        "# This function is based on code from:\n",
        "#   https://www.pluralsight.com/guides/building-a-twitter-sentiment-analysis-in-python\n",
        "def preprocess_text(tweet):\n",
        "    # Changes emojis to words\n",
        "    tweet = emoji.demojize(tweet,  delimiters=(' ', ' '))\n",
        "    # Removes 'RT' from tweet\n",
        "    tweet = re.sub(r'RT[\\s]+', '', tweet)\n",
        "    # Removes capitalization\n",
        "    tweet = tweet.lower()\n",
        "    # Removes urls & user mentions from tweet\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+|\\@\\w+\", ' ', tweet, flags=re.MULTILINE)\n",
        "    # Removes punctuation\n",
        "    tweet = re.sub(r'\\p{P}+', '', tweet)\n",
        "    # Removes stopwords\n",
        "    tokens = [w for w in word_tokenize(tweet) if not w in stop_words]\n",
        "    # Perfoms lemmatization on tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
        "    return \" \".join(lemma_words)\n",
        "\n",
        "# Preprocesses the text of the Tweets in the df and returns the df\n",
        "# By default, this removes the Tweets with the \"neither\" label\n",
        "def preprocess_df(df, remove_neither=True):\n",
        "  idx = \"text\"\n",
        "  length = len(df[idx])\n",
        "  for ii in range(length):\n",
        "    tweet = str(df[idx][ii])\n",
        "    df.loc[ii, idx] = preprocess_text(tweet)\n",
        "  if (remove_neither):\n",
        "    return df[df['BLM'] != \"neither\"]\n",
        "  else:\n",
        "    return df"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow-WoTXPkSts",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fcde5184-b287-4b4e-d332-cb7803d24f6d"
      },
      "source": [
        "# Retrieves and preprocesses the training dataset\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/train.csv\" # Path to train.csv\n",
        "train_df = pd.read_csv(path)\n",
        "train_df.fillna(\"\", inplace=True) # fills any NaN values with empty strings\n",
        "train_df = preprocess_df(train_df)\n",
        "train_df.head(5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>text</th>\n",
              "      <th>BLM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2013-08-05</td>\n",
              "      <td>BlackLivesMatter BrownLivesMatter Every28Hours...</td>\n",
              "      <td>let talk state violence youth color blacklives...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2013-08-30</td>\n",
              "      <td>blacklivesmatter</td>\n",
              "      <td>mt show kid positive image black people build ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013-08-30</td>\n",
              "      <td>BlackLivesMatter BBW13</td>\n",
              "      <td>q1 big parent influence blacklivesmatter bbw13</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2013-08-30</td>\n",
              "      <td>BlackLivesMatter</td>\n",
              "      <td>a10 breastfeeding life love crucial beautiful ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2013-08-30</td>\n",
              "      <td>BlackLivesMatter BBW13</td>\n",
              "      <td>new people jumping cohosting blacklivesmatter ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   created_at  ...       BLM\n",
              "0  2013-08-05  ...  positive\n",
              "2  2013-08-30  ...  positive\n",
              "3  2013-08-30  ...  positive\n",
              "4  2013-08-30  ...  positive\n",
              "5  2013-08-30  ...  positive\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noEV2VuUgzmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a60c8c-2d66-48f7-b545-92d134b20a49"
      },
      "source": [
        "# Uses a CountVectorizer to construct bag-of-words matrix\n",
        "vectorizer = CountVectorizer() # Add a comment about the max_features & ngram_range parameters\n",
        "# train_vocab is an 2d array of the vocab from the training dataset \n",
        "train_vocab = vectorizer.fit_transform(train_df['text']).toarray()\n",
        "# train_vocab_df is a dataframe where the element ij is the number of times word j occurred in Tweet i\n",
        "train_vocab_df = pd.DataFrame(train_vocab, columns=vectorizer.get_feature_names())\n",
        "train_labels = train_df['BLM']\n",
        "print(train_labels)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       positive\n",
            "2       positive\n",
            "3       positive\n",
            "4       positive\n",
            "5       positive\n",
            "          ...   \n",
            "7227    positive\n",
            "7228    positive\n",
            "7229    positive\n",
            "7230    positive\n",
            "7231    positive\n",
            "Name: BLM, Length: 6747, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXTcy6d5Qq3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "74777462-6231-41d8-e99a-a14ba36a78e4"
      },
      "source": [
        "train_vocab_df.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>02125</th>\n",
              "      <th>03</th>\n",
              "      <th>07</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>10000</th>\n",
              "      <th>100000</th>\n",
              "      <th>10003</th>\n",
              "      <th>1000th</th>\n",
              "      <th>100letterstomaketheworldbetter</th>\n",
              "      <th>100plus</th>\n",
              "      <th>100reasons</th>\n",
              "      <th>100yr</th>\n",
              "      <th>101</th>\n",
              "      <th>1010</th>\n",
              "      <th>101015</th>\n",
              "      <th>1024</th>\n",
              "      <th>1029</th>\n",
              "      <th>1030am</th>\n",
              "      <th>1032</th>\n",
              "      <th>10456</th>\n",
              "      <th>1047fm</th>\n",
              "      <th>105</th>\n",
              "      <th>109milelong</th>\n",
              "      <th>10mostfascinatingpeople</th>\n",
              "      <th>10pm</th>\n",
              "      <th>10th</th>\n",
              "      <th>10x</th>\n",
              "      <th>10yr</th>\n",
              "      <th>11</th>\n",
              "      <th>110</th>\n",
              "      <th>1100</th>\n",
              "      <th>110000</th>\n",
              "      <th>1110c</th>\n",
              "      <th>113014</th>\n",
              "      <th>1130am</th>\n",
              "      <th>117</th>\n",
              "      <th>11915</th>\n",
              "      <th>11am</th>\n",
              "      <th>...</th>\n",
              "      <th>yourewelcome</th>\n",
              "      <th>yourlifematters</th>\n",
              "      <th>yourseves</th>\n",
              "      <th>youth</th>\n",
              "      <th>youthdevelopment</th>\n",
              "      <th>youthisthetruth</th>\n",
              "      <th>youthviolence</th>\n",
              "      <th>youtube</th>\n",
              "      <th>youve</th>\n",
              "      <th>yoyo</th>\n",
              "      <th>yr</th>\n",
              "      <th>ystrday</th>\n",
              "      <th>yu</th>\n",
              "      <th>yung</th>\n",
              "      <th>yup</th>\n",
              "      <th>yuvette</th>\n",
              "      <th>zachary</th>\n",
              "      <th>zacharyhammond</th>\n",
              "      <th>zacharyhammonds</th>\n",
              "      <th>zapiro</th>\n",
              "      <th>zaria</th>\n",
              "      <th>zemirbegic</th>\n",
              "      <th>zero</th>\n",
              "      <th>zi</th>\n",
              "      <th>zilphia</th>\n",
              "      <th>zimbabwe</th>\n",
              "      <th>zimmerman</th>\n",
              "      <th>zind</th>\n",
              "      <th>zinnbookfest</th>\n",
              "      <th>zion</th>\n",
              "      <th>zionist</th>\n",
              "      <th>zipper</th>\n",
              "      <th>ziptied</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zone</th>\n",
              "      <th>zone17</th>\n",
              "      <th>zubat</th>\n",
              "      <th>zurbanotorres</th>\n",
              "      <th>zwarte</th>\n",
              "      <th>zwartepiet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 10616 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   02125  03  07  10  100  ...  zone17  zubat  zurbanotorres  zwarte  zwartepiet\n",
              "0      0   0   0   0    0  ...       0      0              0       0           0\n",
              "1      0   0   0   0    0  ...       0      0              0       0           0\n",
              "2      0   0   0   0    0  ...       0      0              0       0           0\n",
              "3      0   0   0   0    0  ...       0      0              0       0           0\n",
              "4      0   0   0   0    0  ...       0      0              0       0           0\n",
              "\n",
              "[5 rows x 10616 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHgyWq_RIroz"
      },
      "source": [
        "# Retrieves and preprocesses the test dataset\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/test.csv\" # Path to Test_dataset.csv\n",
        "test_df = pd.read_csv(path)\n",
        "test_df.fillna(\"\", inplace=True) # fills any NaN values with empty strings\n",
        "test_df = preprocess_df(test_df)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs9kGIt-_J22"
      },
      "source": [
        "# Uses the vocab from the training dataset to vectorize the test dataset\n",
        "test_vocab = vectorizer.transform(test_df['text']).toarray()\n",
        "# test_vocab_df is a dataframe where the element ij is the number of times word j\n",
        "# occurred in Tweet i\n",
        "test_vocab_df = pd.DataFrame(test_vocab, columns=vectorizer.get_feature_names())\n",
        "test_labels = test_df['BLM']"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xW0Zv5PAf1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87afacf7-da67-49d8-c81e-d97059d21047"
      },
      "source": [
        "print(f\"Number of neither Tweets in training: {len(train_df[train_df['BLM'] == 'neither'])}\")\n",
        "print(f\"Number of positive Tweets in training: {len(train_df[train_df['BLM'] == 'positive'])}\")\n",
        "print(f\"Number of negative Tweets in training: {len(train_df[train_df['BLM'] == 'negative'])}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of neither Tweets in training: 0\n",
            "Number of positive Tweets in training: 5528\n",
            "Number of negative Tweets in training: 1219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Funzv_LhBBPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9044b8d8-7c53-49ea-9fbd-1ef4e208d2bb"
      },
      "source": [
        "print(f\"Number of neither Tweets in test: {len(test_df[test_df['BLM'] == 'neither'])}\")\n",
        "print(f\"Number of positive Tweets in test: {len(test_df[test_df['BLM'] == 'positive'])}\")\n",
        "print(f\"Number of negative Tweets in test: {len(test_df[test_df['BLM'] == 'negative'])}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of neither Tweets in test: 0\n",
            "Number of positive Tweets in test: 1383\n",
            "Number of negative Tweets in test: 305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGeJNG-fHJK9"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW6EHnAbfZpN"
      },
      "source": [
        "train_labels = train_labels.replace('positive', 1)\n",
        "train_labels = train_labels.replace('negative', 0)\n",
        "\n",
        "test_labels = test_labels.replace('positive', 1)\n",
        "test_labels = test_labels.replace('negative', 0)\n",
        "\n",
        "train_labels_arr = np.array(train_labels).reshape(len(train_labels),)\n",
        "test_labels_arr = np.array(test_labels).reshape(len(test_labels),)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgkGS9OIQpXk"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import cross_validate"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feICnpjaKlU5"
      },
      "source": [
        "import statistics\n",
        "import time\n",
        "\n",
        "def cross_validation(classifier, numfolds):\n",
        "  cv_results_accuracy = \\\n",
        "    cross_validate(classifier, train_vocab_df, train_labels_arr, cv=numfolds, scoring='accuracy')\n",
        "  cv_results_recall = \\\n",
        "    cross_validate(classifier, train_vocab_df, train_labels_arr, cv=numfolds, scoring='recall')\n",
        "  cv_results_f1 = \\\n",
        "    cross_validate(classifier, train_vocab_df, train_labels_arr, cv=numfolds, scoring='f1')\n",
        "  cv_results_precision = \\\n",
        "    cross_validate(classifier, train_vocab_df, train_labels_arr, cv=numfolds, scoring='precision')\n",
        "\n",
        "  print(\"accuracy cv: \", statistics.mean(cv_results_accuracy['test_score']))\n",
        "  print(\"precision cv: \", statistics.mean(cv_results_precision['test_score']))\n",
        "  print(\"recall cv: \", statistics.mean(cv_results_recall['test_score']))\n",
        "  print(\"f1 cv: \", statistics.mean(cv_results_f1['test_score']))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBg-1SDWQlEp",
        "outputId": "b79c2161-c385-465b-db55-6b9fed7cbf55"
      },
      "source": [
        "# GaussianNB\n",
        "gnb = GaussianNB()\n",
        "\n",
        "start = time.time()\n",
        "prediction = gnb.fit(train_vocab_df, train_labels_arr).predict(test_vocab_df)\n",
        "end = time.time()\n",
        "\n",
        "print(\"fit and predict time (GaussianNB): \" + str(end-start))\n",
        "\n",
        "print(\"Accuracy score Gaussian NB: \", gnb.score(test_vocab_df, test_labels_arr))\n",
        "print(\"Precision Score: \", precision_score(test_labels_arr, prediction))\n",
        "print(\"F1 Score: \", f1_score(test_labels_arr, prediction))\n",
        "print(\"Recall Score: \", recall_score(test_labels_arr, prediction))\n",
        "\n",
        "print(\"5-fold\")\n",
        "cross_validation(gnb, 5)\n",
        "print(\"10-fold\")\n",
        "cross_validation(gnb, 10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fit and predict time (GaussianNB): 1.2691256999969482\n",
            "Accuracy score Gaussian NB:  0.6741706161137441\n",
            "Precision Score:  0.8748874887488749\n",
            "F1 Score:  0.7794707297514033\n",
            "Recall Score:  0.702819956616052\n",
            "5-fold\n",
            "accuracy cv:  0.6678446036844851\n",
            "precision cv:  0.8662981229635603\n",
            "recall cv:  0.703136982154108\n",
            "f1 cv:  0.7756907434879029\n",
            "10-fold\n",
            "accuracy cv:  0.6727354654357621\n",
            "precision cv:  0.8719077947958584\n",
            "recall cv:  0.7040418533951831\n",
            "f1 cv:  0.7785535542837063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_asopdWYlBG5",
        "outputId": "e94644f2-1206-446f-9873-2f342721dcf9"
      },
      "source": [
        "# MultinomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "start = time.time()\n",
        "prediction = mnb.fit(train_vocab_df, train_labels_arr).predict(test_vocab_df)\n",
        "end = time.time()\n",
        "\n",
        "print(\"fit and predict time (MultinomialNB): \" + str(end-start))\n",
        "\n",
        "print(\"Accuracy score Multinomial NB: \", mnb.score(test_vocab_df, test_labels_arr))\n",
        "print(\"Precision Score: \", precision_score(test_labels_arr, prediction))\n",
        "print(\"F1 Score: \", f1_score(test_labels_arr, prediction))\n",
        "print(\"Recall Score: \", recall_score(test_labels_arr, prediction))\n",
        "\n",
        "print(\"5-fold\")\n",
        "cross_validation(mnb, 5)\n",
        "print(\"10-fold\")\n",
        "cross_validation(mnb, 10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fit and predict time (MultinomialNB): 0.5606226921081543\n",
            "Accuracy score Multinomial NB:  0.840047393364929\n",
            "Precision Score:  0.8560460652591171\n",
            "F1 Score:  0.9083503054989818\n",
            "Recall Score:  0.9674620390455532\n",
            "5-fold\n",
            "accuracy cv:  0.795460450814046\n",
            "precision cv:  0.8551301017782915\n",
            "recall cv:  0.9039336240825444\n",
            "f1 cv:  0.8784435151661955\n",
            "10-fold\n",
            "accuracy cv:  0.797088251456204\n",
            "precision cv:  0.8594814112893662\n",
            "recall cv:  0.8997697014964489\n",
            "f1 cv:  0.8788065120712019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L84bd-NYnN_T",
        "outputId": "6fa3906a-138e-493d-d036-4896171228ce"
      },
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "start = time.time()\n",
        "lrclf = LogisticRegression().fit(train_vocab_df, train_labels_arr)\n",
        "prediction = lrclf.predict(test_vocab_df)\n",
        "end = time.time()\n",
        "\n",
        "print(\"fit and predict time (Logistic Regression): \" + str(end-start))\n",
        "\n",
        "print(\"Accuracy score Logistic Regression: \", lrclf.score(test_vocab_df, test_labels_arr))\n",
        "print(\"Precision Score: \", precision_score(test_labels_arr, prediction))\n",
        "print(\"F1 Score: \", f1_score(test_labels_arr, prediction))\n",
        "print(\"Recall Score: \", recall_score(test_labels_arr, prediction))\n",
        "\n",
        "print(\"5-fold\")\n",
        "cross_validation(lrclf, 5)\n",
        "print(\"10-fold\")\n",
        "cross_validation(lrclf, 10)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fit and predict time (Logistic Regression): 13.235511541366577\n",
            "Accuracy score Logistic Regression:  0.840047393364929\n",
            "Precision Score:  0.8668424522083059\n",
            "F1 Score:  0.906896551724138\n",
            "Recall Score:  0.9508315256688359\n",
            "5-fold\n",
            "accuracy cv:  0.8259900612250501\n",
            "precision cv:  0.8582722686789126\n",
            "recall cv:  0.9435482313665485\n",
            "f1 cv:  0.8986518521252143\n",
            "10-fold\n",
            "accuracy cv:  0.8286565556654577\n",
            "precision cv:  0.8595245877065946\n",
            "recall cv:  0.945539481615431\n",
            "f1 cv:  0.9003336350895486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp9zHfO4saTV",
        "outputId": "b0a71417-3f4d-4b4a-c778-f8087d783fd3"
      },
      "source": [
        "# MLP with logistic activiation\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_logistic = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                     hidden_layer_sizes=(5, 2), random_state=1)\n",
        "\n",
        "start = time.time()\n",
        "mlp_logistic.fit(train_vocab_df, train_labels_arr)\n",
        "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1, solver='lbfgs', activation='logistic')\n",
        "prediction = mlp_logistic.predict(test_vocab_df)\n",
        "end = time.time()\n",
        "\n",
        "print(\"fit and predict time (MLP with logistic activiation): \" + str(end-start))\n",
        "\n",
        "print(\"Accuracy score MLP: \", mlp_logistic.score(test_vocab_df, test_labels_arr))\n",
        "print(\"Precision Score: \", precision_score(test_labels_arr, prediction))\n",
        "print(\"F1 Score: \", f1_score(test_labels_arr, prediction))\n",
        "print(\"Recall Score: \", recall_score(test_labels_arr, prediction))\n",
        "\n",
        "print(\"5-fold\")\n",
        "cross_validation(mlp_logistic, 5)\n",
        "print(\"10-fold\")\n",
        "cross_validation(mlp_logistic, 10)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fit and predict time (MLP with logistic activiation): 6.605457067489624\n",
            "Accuracy score MLP:  0.8193127962085308\n",
            "Precision Score:  0.8193127962085308\n",
            "F1 Score:  0.900683816346467\n",
            "Recall Score:  1.0\n",
            "5-fold\n",
            "accuracy cv:  0.8193271284627845\n",
            "precision cv:  0.8193271284627845\n",
            "recall cv:  1.0\n",
            "f1 cv:  0.900692451166004\n",
            "10-fold\n",
            "accuracy cv:  0.8193271788108583\n",
            "precision cv:  0.8193271788108583\n",
            "recall cv:  1.0\n",
            "f1 cv:  0.900692454648104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh8EKkZ92mpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55c58cc-ac45-4c78-9b50-d2099f36211a"
      },
      "source": [
        "# MLP with relu activation\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_relu = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                     hidden_layer_sizes=(5, 2), random_state=1)\n",
        "\n",
        "start = time.time()\n",
        "mlp_relu.fit(train_vocab_df, train_labels_arr)\n",
        "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1, solver='lbfgs', activation='relu')\n",
        "prediction = mlp_relu.predict(test_vocab_df)\n",
        "end = time.time()\n",
        "\n",
        "print(\"fit and predict time (MLP with relu activation): \" + str(end-start))\n",
        "\n",
        "print(\"Accuracy score MLP: \", mlp_relu.score(test_vocab_df, test_labels_arr))\n",
        "print(\"Precision Score: \", precision_score(test_labels_arr, prediction))\n",
        "print(\"F1 Score: \", f1_score(test_labels_arr, prediction))\n",
        "print(\"Recall Score: \", recall_score(test_labels_arr, prediction))\n",
        "\n",
        "print(\"5-fold\")\n",
        "cross_validation(mlp_relu, 5)\n",
        "print(\"10-fold\")\n",
        "cross_validation(mlp_relu, 10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fit and predict time (MLP with relu activation): 6.506883382797241\n",
            "Accuracy score MLP:  0.8193127962085308\n",
            "Precision Score:  0.8193127962085308\n",
            "F1 Score:  0.900683816346467\n",
            "Recall Score:  1.0\n",
            "5-fold\n",
            "accuracy cv:  0.8193271284627845\n",
            "precision cv:  0.8193271284627845\n",
            "recall cv:  1.0\n",
            "f1 cv:  0.900692451166004\n",
            "10-fold\n",
            "accuracy cv:  0.8193271788108583\n",
            "precision cv:  0.8193271788108583\n",
            "recall cv:  1.0\n",
            "f1 cv:  0.900692454648104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh3i3Vcb4Iu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6bd918-33bd-4bfd-db69-30aecf15753f"
      },
      "source": [
        "# DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dtclf = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "start = time.time()\n",
        "prediction = dtclf.fit(train_vocab_df, train_labels_arr).predict(test_vocab_df)\n",
        "end = time.time()\n",
        "\n",
        "print(\"fit and predict time (DecisionTree): \" + str(end-start))\n",
        "\n",
        "print(\"Accuracy score DecisionTreeClassifier: \", mlp_relu.score(test_vocab_df, test_labels_arr))\n",
        "print(\"Precision Score: \", precision_score(test_labels_arr, prediction))\n",
        "print(\"F1 Score: \", f1_score(test_labels_arr, prediction))\n",
        "print(\"Recall Score: \", recall_score(test_labels_arr, prediction))\n",
        "\n",
        "print(\"5-fold\")\n",
        "cross_validation(dtclf, 5)\n",
        "print(\"10-fold\")\n",
        "cross_validation(dtclf, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fit and predict time (DecisionTree): 147.91907000541687\n",
            "Accuracy score DecisionTreeClassifier:  0.8193127962085308\n",
            "Precision Score:  0.8806290207290922\n",
            "F1 Score:  0.8856937455068297\n",
            "Recall Score:  0.8908170643528561\n",
            "5-fold\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}